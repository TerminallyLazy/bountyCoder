worker_processes auto;

events {
    worker_connections 1024;
}

http {
    upstream llm_servers {
        # Use least_conn for better load distribution
        least_conn;
        
        # LLM service instances
        server llm-service:8000;
        
        # Add more instances here for scaling
        # server llm-service-2:8000;
        # server llm-service-3:8000;
    }
    
    server {
        listen 80;
        
        location / {
            proxy_pass http://llm_servers;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeout settings for long-running LLM requests
            proxy_connect_timeout 300s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
            
            # WebSocket support for streaming responses
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }
        
        # Health check endpoint
        location /health {
            proxy_pass http://llm_servers/health;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            
            # More frequent health checks
            health_check interval=10s fails=3 passes=2;
        }
        
        # Prometheus metrics endpoint
        location /metrics {
            proxy_pass http://llm_servers:8001/metrics;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }
    }
}
